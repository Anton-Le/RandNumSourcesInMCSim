{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16306731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os, shutil, sys, re\n",
    "from scipy import stats\n",
    "from tabulate import tabulate\n",
    "import math\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a536bb6",
   "metadata": {},
   "source": [
    "## Defining data files and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec1fca-1fff-4798-ba3c-3fd96bc47125",
   "metadata": {},
   "source": [
    "Define directories where the data is stored and where thew results (plots) are to be written to, along with experimental parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ecb27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRNG_dir = \"../experimentData/PointDistributionData/CollectedData_uniformity/PRNG/\"\n",
    "QRNG_dir = \"../experimentData/PointDistributionData/CollectedData_uniformity/QRNG/\"\n",
    "\n",
    "precision =  'double'#'single'#\n",
    "\n",
    "alpha = 0.05 # significance\n",
    "NSeeds = 50\n",
    "\n",
    "Results_directory = \"../CollectedData_uniformity/Plots\"\n",
    "Table_digits = '.4f'\n",
    "\n",
    "\n",
    "Nsamples = ['1K', '5K', '10K', '50K']\n",
    "Nsamples_number = np.array([1000, 5000, 10000, 50000])\n",
    "\n",
    "OneSample = '50K'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b54e4a",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7fe420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseResults(filename_LES_statistics:str, filename_NN_statistics:str):\n",
    "    results = {}\n",
    "\n",
    "    data = np.genfromtxt(filename_LES_statistics, delimiter=',')\n",
    "    results['meanLES'] = data[0]\n",
    "    results['varLES'] = data[1]\n",
    "    \n",
    "    data = np.genfromtxt(filename_NN_statistics, delimiter=',')\n",
    "    results['meanNN'] = data[0]\n",
    "    results['varNN'] = data[1]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04ee2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseResults_mean_var(filename_data:str):\n",
    "    data = np.genfromtxt(filename_data, delimiter=',')\n",
    "    results = {}\n",
    "    results['meanQ'] = np.mean(data)\n",
    "    results['varQ'] = np.var(data)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af988c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/TGT/DatavisVENV_V2/lib/python3.8/site-packages/numpy/core/_methods.py:236: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n"
     ]
    }
   ],
   "source": [
    "if precision=='double':\n",
    "    ResultFilenameTemplate_les = \"vis_{Samples}/lesStatistics_double_{Nrep:d}.dat\"\n",
    "    ResultFilenameTemplate_nn = \"vis_{Samples}/nnStatistics_double_{Nrep:d}.dat\"\n",
    "    ResultFilenameTemplate_q = \"vis_{Samples}/nndata_PE_Q_double_{Nrep:d}.dat\"\n",
    "    ResultFilenameTemplate_CoV = \"vis_{Samples}/nndata_CoV_double_{Nrep:d}.dat\"\n",
    "    ResultFilenameTemplate_Avg = \"vis_{Samples}/nndata_Avg_double_{Nrep:d}.dat\"\n",
    "    ResultFilenameTemplate_Rm = \"vis_{Samples}/nndata_Rm_double_{Nrep:d}.dat\"\n",
    "elif precision=='single':\n",
    "    ResultFilenameTemplate_les = \"vis_{Samples}/lesStatistics_single_{Nrep:d}.dat\"\n",
    "    ResultFilenameTemplate_nn = \"vis_{Samples}/nnStatistics_single_{Nrep:d}.dat\"\n",
    "    ResultFilenameTemplate_q = \"vis_{Samples}/nndata_PE_Q_single_{Nrep:d}.dat\"\n",
    "    ResultFilenameTemplate_CoV = \"vis_{Samples}/nndata_CoV_single_{Nrep:d}.dat\"\n",
    "    ResultFilenameTemplate_Avg = \"vis_{Samples}/nndata_Avg_single_{Nrep:d}.dat\"\n",
    "    ResultFilenameTemplate_Rm = \"vis_{Samples}/nndata_Rm_single_{Nrep:d}.dat\"\n",
    "\n",
    "\n",
    "# Define array names\n",
    "array_names = ['qrng_les_means', 'qrng_les_variances', 'prng_les_means', 'prng_les_variances',\n",
    "               'qrng_nn_means', 'qrng_nn_variances', 'prng_nn_means', 'prng_nn_variances',\n",
    "               'qrng_q_means', 'qrng_q_variances', 'prng_q_means', 'prng_q_variances',\n",
    "               'qrng_Avg_means', 'qrng_Avg_variances', 'prng_Avg_means', 'prng_Avg_variances',\n",
    "               'qrng_CoV_means', 'qrng_CoV_variances', 'prng_CoV_means', 'prng_CoV_variances',\n",
    "               'qrng_Rm_means', 'qrng_Rm_variances', 'prng_Rm_means', 'prng_Rm_variances',\n",
    "               'qrng_CE', 'prng_CE', 'qrng_R_nr', 'prng_R_nr']\n",
    "\n",
    "# Initialize arrays using a loop\n",
    "arrays = {name: np.zeros((NSeeds, len(Nsamples)), dtype=float) for name in array_names}\n",
    "\n",
    "#iterate over the batch sizes\n",
    "for repIdx in range(NSeeds):\n",
    "    for Sample, nSamplesIdx in zip(Nsamples, range(len(Nsamples)) ):\n",
    "        #print(\"NSeeds\", repIdx, \"Sample\",Sample, \"SampleIdx\", nSamplesIdx)\n",
    "        filenameParameters= {'Samples':Sample, 'Nrep':repIdx+1}\n",
    "        filename_les = ResultFilenameTemplate_les.format(**filenameParameters)\n",
    "        filename_nn = ResultFilenameTemplate_nn.format(**filenameParameters)\n",
    "        filename_q = ResultFilenameTemplate_q.format(**filenameParameters)\n",
    "        filename_Avg = ResultFilenameTemplate_Avg.format(**filenameParameters)\n",
    "        filename_Rm = ResultFilenameTemplate_Rm.format(**filenameParameters)\n",
    "        filename_CoV = ResultFilenameTemplate_CoV.format(**filenameParameters)\n",
    "        \n",
    "        data = parseResults(QRNG_dir +filename_les, QRNG_dir +filename_nn)\n",
    "        \n",
    "        data_q = parseResults_mean_var(QRNG_dir +filename_q)\n",
    "        data_Avg = parseResults_mean_var(QRNG_dir +filename_Avg)\n",
    "        data_Rm = parseResults_mean_var(QRNG_dir +filename_Rm)\n",
    "        data_CoV = parseResults_mean_var(QRNG_dir +filename_CoV)\n",
    "\n",
    "        data_dicts_qrng = {'qrng_q': data_q, 'qrng_Rm': data_Rm, 'qrng_Avg': data_Avg, 'qrng_CoV': data_CoV}\n",
    "        \n",
    "        #distribute data to the arrays\n",
    "                \n",
    "        arrays['qrng_les_means'][repIdx, nSamplesIdx] = data['meanLES']\n",
    "        arrays['qrng_les_variances'][repIdx, nSamplesIdx] = data['varLES']\n",
    "        \n",
    "        arrays['qrng_nn_means'][repIdx, nSamplesIdx] = data['meanNN']\n",
    "        arrays['qrng_nn_variances'][repIdx, nSamplesIdx] = data['varNN']\n",
    "        \n",
    "        for prefix, data in data_dicts_qrng.items():\n",
    "            arrays[f\"{prefix}_means\"][repIdx, nSamplesIdx] = data['meanQ']\n",
    "            arrays[f\"{prefix}_variances\"][repIdx, nSamplesIdx] = data['varQ']\n",
    "        \n",
    "        \n",
    "        #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "        ##                          PRNG\n",
    "        #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "        data = parseResults(PRNG_dir + filename_les, PRNG_dir + filename_nn)\n",
    "        \n",
    "        data_q = parseResults_mean_var(PRNG_dir +filename_q)\n",
    "        data_Avg = parseResults_mean_var(PRNG_dir +filename_Avg)\n",
    "        data_Rm = parseResults_mean_var(PRNG_dir +filename_Rm)\n",
    "        data_CoV = parseResults_mean_var(PRNG_dir +filename_CoV)\n",
    "        \n",
    "        data_dicts_prng = {'prng_q': data_q, 'prng_Rm': data_Rm, 'prng_Avg': data_Avg, 'prng_CoV': data_CoV}\n",
    "\n",
    "        #distribute data to the arrays\n",
    "        arrays['prng_les_means'][repIdx, nSamplesIdx] = data['meanLES']\n",
    "        arrays['prng_les_variances'][repIdx, nSamplesIdx] = data['varLES']\n",
    "        \n",
    "        arrays['prng_nn_means'][repIdx, nSamplesIdx] = data['meanNN']\n",
    "        arrays['prng_nn_variances'][repIdx, nSamplesIdx] = data['varNN']\n",
    "        \n",
    "        for prefix, data in data_dicts_prng.items():\n",
    "            arrays[f\"{prefix}_means\"][repIdx, nSamplesIdx] = data['meanQ']\n",
    "            arrays[f\"{prefix}_variances\"][repIdx, nSamplesIdx] = data['varQ']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb3efb",
   "metadata": {},
   "source": [
    "#### Functions to create plots and statistics tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9429a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_statistic(Nsamples_number, qrng_data, prng_data, test_type,precision,test):\n",
    "    fig, axes = plt.subplots(1, len(Nsamples_number), figsize=(12, 5))\n",
    "    for Sample, nSamplesIdx in zip(Nsamples_number, range(len(Nsamples_number)) ):\n",
    "        bp = axes[nSamplesIdx].boxplot([qrng_data[:, nSamplesIdx], prng_data[:, nSamplesIdx]], patch_artist=True, showmeans=True, meanline=True, medianprops=dict(color='red'))\n",
    "        colors = ['blue', 'orange']\n",
    "        for box, color in zip(bp['boxes'], colors):\n",
    "            box.set(color=color, linewidth=2)\n",
    "            box.set(facecolor='lightgrey')        \n",
    "        axes[nSamplesIdx].set_title('${}$ test, {} points'.format(test_type, Sample))\n",
    "\n",
    "        axes[nSamplesIdx].set_xticks([])\n",
    "    \n",
    "    legend_colors = ['blue', 'orange']\n",
    "    legend_labels = ['QRNG', 'PRNG']\n",
    "\n",
    "    # Creating legend patches\n",
    "    legend_patches = [mpatches.Patch(color=color, label=label) for color, label in zip(legend_colors, legend_labels)]\n",
    "    legend_elements = [Line2D([0], [0], color='red', lw=2, label='Median'), \n",
    "                    Line2D([0], [0], color='green', lw=2, linestyle='--', label='Mean')]\n",
    "    all_handles = legend_elements + legend_patches\n",
    "    plt.legend(handles=all_handles, loc='upper center', fontsize='small', bbox_to_anchor=(0.5, 0.0), ncol=4)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Construct the file path\n",
    "    file_path = os.path.join(Results_directory, test + '_'+precision+'.png')\n",
    "    plt.savefig(file_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085964ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test\n",
    "def T_test(case1_means,prngcase1_means):\n",
    "    PoolVariance = 1.0 / (len(case1_means) + len(prngcase1_means) - 2) * (\n",
    "                (len(case1_means) - 1) * np.std(case1_means, ddof=1) ** 2 + (\n",
    "                    len(prngcase1_means) - 1) * np.std(prngcase1_means, ddof=1) ** 2)\n",
    "    t_samples = (np.mean(case1_means) - np.mean(prngcase1_means)) / (\n",
    "                np.sqrt(PoolVariance * (1.0 / len(prngcase1_means) + 1.0 / len(case1_means))))\n",
    "    tDOF_joint = len(case1_means) + len(prngcase1_means) - 2\n",
    "    t_1_joint = stats.t.ppf(alpha / 2, tDOF_joint)\n",
    "    t_2_joint = stats.t.ppf(1 - alpha / 2, tDOF_joint)\n",
    "    \n",
    "    # Calculate the p-value for a two-tailed test\n",
    "    p_value = 2 * min(stats.t.cdf(t_samples, tDOF_joint), 1 - stats.t.cdf(t_samples, tDOF_joint))\n",
    "    \n",
    "    # Determine the test result based on the p-value\n",
    "    if t_samples < t_1_joint:\n",
    "        test_result_T_test = 'Significant'\n",
    "    elif t_samples > t_2_joint:\n",
    "        test_result_T_test = 'Significant'\n",
    "    else:\n",
    "        test_result_T_test = 'Not Significant'\n",
    "    return t_samples, p_value, test_result_T_test, t_1_joint,t_2_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "832c17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_test(case1_means,prngcase1_means):\n",
    "    f = np.var(case1_means, ddof=1) / np.var(prngcase1_means, ddof=1)\n",
    "    f_1 = stats.f.ppf(alpha / 2, len(case1_means) - 1, len(prngcase1_means) - 1)\n",
    "    f_2 = stats.f.ppf(1 - alpha / 2, len(case1_means) - 1, len(prngcase1_means) - 1)\n",
    "    if f < f_1:\n",
    "        test_result_F_test = 'Significant'\n",
    "    elif f > f_2:\n",
    "        test_result_F_test = 'Significant'\n",
    "    else:\n",
    "        test_result_F_test = 'Not Significant'\n",
    "    \n",
    "    \n",
    "    p_value_f = stats.f.sf(f, len(case1_means) - 1, len(prngcase1_means) - 1)\n",
    "    return f, p_value_f, test_result_F_test, f_1, f_2\n",
    "\n",
    "    #----- For a two-tailed interpretation, you might double the p-value (not typical for F-tests focusing on variance comparisons):\n",
    "    #----- p_value_f_two_tailed = 2 * min(p_value_f, 1 - p_value_f)\n",
    "\n",
    "    #p_value_f_two_tailed = 2 * min(p_value_f, 1 - p_value_f)\n",
    "    #return f, p_value_f_two_tailed, test_result_F_test, f_1, f_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0fc5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statistical_tests(test,precision, Nsamples, qrng_means, prng_means,alpha):\n",
    "    # Example data (replace this with your own data)\n",
    "    data_Wlcx = {\n",
    "        'Sample Size': [],\n",
    "        'Wilcoxon Statistic': [],\n",
    "        'P-value': [],\n",
    "        'Test Result': []  \n",
    "    }\n",
    "\n",
    "    data_Ttest = {\n",
    "        'Sample Size': [],\n",
    "        'Test statistic': [],\n",
    "        'P-value': [],\n",
    "        'Test Result': []  \n",
    "    }\n",
    "\n",
    "    data_Ftest = {\n",
    "        'Sample Size':[],\n",
    "        'Test statistic': [],\n",
    "        'P-value': [],\n",
    "        'Test Result': []  \n",
    "    }\n",
    "\n",
    "    data_KStest = {\n",
    "        'Sample Size': [],\n",
    "        'Test statistic': [],\n",
    "        'P-value': [],\n",
    "        'Test Result': []  \n",
    "    }\n",
    "\n",
    "    data_ADtest = {\n",
    "        'Sample Size': [],\n",
    "        'Test statistic': [],\n",
    "        'P-value': [],\n",
    "        'Test Result': []  \n",
    "    }\n",
    "    \n",
    "    data_ALL_Tests = {\n",
    "        'W test':[],\n",
    "        'T test': [],\n",
    "        'F test': [],\n",
    "        'AD test': [],\n",
    "        'KS test': []\n",
    "    }\n",
    "\n",
    "    Sample_as_number = Nsamples\n",
    "    \n",
    "    print('alpha=',alpha)\n",
    "\n",
    "    for Sample, nSamplesIdx in zip(Nsamples, range(len(Nsamples))):\n",
    "        case1_means = qrng_means[:, nSamplesIdx]\n",
    "        prngcase1_means = prng_means[:, nSamplesIdx]\n",
    "\n",
    "        statistic, p_value = stats.wilcoxon(case1_means, prngcase1_means)\n",
    "        # Append results to the data dictionary\n",
    "        data_Wlcx['Sample Size'].append(Sample_as_number[nSamplesIdx])\n",
    "\n",
    "        data_Wlcx['Wilcoxon Statistic'].append(statistic)\n",
    "        data_Wlcx['P-value'].append(format(p_value, Table_digits))\n",
    "\n",
    "        # Determine the test result based on the p-value\n",
    "        test_result_Wlcx = 'Significant' if p_value < alpha else 'Not Significant'\n",
    "        data_Wlcx['Test Result'].append(test_result_Wlcx)\n",
    "\n",
    "        # T-test\n",
    "        [t_samples, p_value, test_result_T_test,t_1_joint,t_2_joint] = T_test(case1_means,prngcase1_means)\n",
    "        \n",
    "        data_Ttest['P-value'].append(format(p_value, Table_digits))\n",
    "        data_Ttest['Test statistic'].append(format(t_samples, Table_digits))\n",
    "        data_Ttest['Sample Size'].append(Sample_as_number[nSamplesIdx])\n",
    "        data_Ttest['Test Result'].append(test_result_T_test)\n",
    "\n",
    "        # F-test        \n",
    "        [f, p_value_f, test_result_F_test, f_1, f_2] = F_test(case1_means,prngcase1_means)\n",
    "        data_Ftest['Sample Size'].append(Sample_as_number[nSamplesIdx])\n",
    "        data_Ftest['Test statistic'].append(format(f, Table_digits))\n",
    "        data_Ftest['P-value'].append(format(p_value_f, Table_digits))\n",
    "        data_Ftest['Test Result'].append(test_result_F_test)\n",
    "\n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_result = stats.ks_2samp(case1_means, prngcase1_means)\n",
    "        ks_statistic = ks_result.statistic\n",
    "        p_value = ks_result.pvalue\n",
    "        test_result_KS = 'Significant' if p_value < alpha else 'Not Significant'\n",
    "\n",
    "        data_KStest['P-value'].append(format(p_value, Table_digits))\n",
    "        data_KStest['Sample Size'].append(Sample_as_number[nSamplesIdx])\n",
    "        data_KStest['Test statistic'].append(format(p_value, Table_digits))\n",
    "        data_KStest['Test Result'].append(test_result_KS)\n",
    "\n",
    "        # Anderson-Darling test\n",
    "        ad_result = stats.anderson_ksamp([case1_means, prngcase1_means])\n",
    "        ad_statistic = ad_result.statistic\n",
    "        p_value = ad_result.pvalue\n",
    "        test_result_AD = 'Significant' if p_value < alpha else 'Not Significant'\n",
    "        \n",
    "        data_ADtest['P-value'].append(format(p_value, Table_digits))\n",
    "        data_ADtest['Sample Size'].append(Sample_as_number[nSamplesIdx])\n",
    "        data_ADtest['Test statistic'].append(format(p_value, Table_digits))\n",
    "        data_ADtest['Test Result'].append(test_result_AD)\n",
    "        \n",
    "        #==================================================================\n",
    "        data_ALL_Tests['W test'].append(data_Wlcx['P-value'][-1])\n",
    "        data_ALL_Tests['KS test'].append(data_KStest['P-value'][-1])\n",
    "        data_ALL_Tests['T test'].append(data_Ttest['P-value'][-1])\n",
    "        data_ALL_Tests['F test'].append(data_Ftest['P-value'][-1])\n",
    "        data_ALL_Tests['AD test'].append(data_ADtest['P-value'][-1])\n",
    "        \n",
    "\n",
    "    print(test, precision, \"|    Wilcoxon test\")\n",
    "    print('Wilcoxon test is the sum of the ranks of the positive (or negative) differences. It represents the degree of difference between the two groups')\n",
    "    table_W = tabulate(data_Wlcx, headers='keys', tablefmt='pretty')\n",
    "    print(table_W)\n",
    "    \n",
    "    print(test, precision, \"|    T-test\")\n",
    "    print([t_1_joint,t_2_joint])\n",
    "    table_T = tabulate(data_Ttest, headers='keys', tablefmt='pretty')\n",
    "    print(table_T)\n",
    "    \n",
    "    print(test, precision, \"|    Anderson-Darling test\")\n",
    "    table_AD = tabulate(data_ADtest, headers='keys', tablefmt='pretty')\n",
    "    print(table_AD)\n",
    "    \n",
    "    print(test, precision, \"|    F-test\")\n",
    "    print([f_1,f_2])\n",
    "    table_F = tabulate(data_Ftest, headers='keys', tablefmt='pretty')\n",
    "    print(table_F)\n",
    "    \n",
    "    print(test, precision, \"|    Kolmogorov-Smirnov test\")\n",
    "    table_KS = tabulate(data_KStest, headers='keys', tablefmt='pretty')\n",
    "    print(table_KS)\n",
    "    \n",
    "    # Print results as a table\n",
    "    if not os.path.exists(Results_directory):\n",
    "        os.makedirs(Results_directory)\n",
    "    file_path = Results_directory +'/'+ test+'_'+precision+'_table.txt'\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(f\"alpha = {alpha}\\n\")        \n",
    "        file.write(f\"{test}, {precision} |    Wilcoxon test\\n\")        \n",
    "        file.write(table_W)\n",
    "        file.write('\\n') \n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(f\"{test}, {precision} |    T-test\\n\")        \n",
    "        file.write(f\"[{t_1_joint},{t_2_joint}]\\n\")        \n",
    "        file.write(table_T)\n",
    "        file.write('\\n') \n",
    "        file.write(f\"{test}, {precision} |    Anderson-Darling test\\n\")        \n",
    "        file.write(table_AD)\n",
    "        file.write('\\n')  \n",
    "        file.write(f\"{test}, {precision} |    F-test\\n\")        \n",
    "        file.write(f\"[{f_1},{f_2}]\\n\")        \n",
    "        file.write(table_F)\n",
    "        file.write('\\n')  \n",
    "        file.write(f\"{test}, {precision} |     Kolmogorov-Smirnov test\\n\")        \n",
    "        file.write(table_KS)\n",
    "        file.write('\\n')  # Add an extra line \n",
    "        \n",
    "    return data_ALL_Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55cd8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_results(test,alpha,precision):\n",
    "    ALL_tests_for_metric={}\n",
    "    if test=='les':\n",
    "        ALL_tests_for_metric = run_statistical_tests(test,precision, Nsamples, arrays['qrng_les_means'], arrays['prng_les_means'],alpha)\n",
    "        fig, axes = plt.subplots(1, len(Nsamples_number), figsize=(12, 5))\n",
    "        for Sample, nSamplesIdx in zip(Nsamples_number, range(len(Nsamples_number)) ):\n",
    "            bp = axes[nSamplesIdx].boxplot([np.pi*(arrays['qrng_les_means'][:, nSamplesIdx])**2, np.pi*(arrays['prng_les_means'][:, nSamplesIdx])**2],patch_artist=True, showmeans=True, meanline=True, medianprops=dict(color='red'))\n",
    "            n = float(Nsamples_number[nSamplesIdx])\n",
    "            test_line = (-math.log(-math.log(1 - alpha)) + math.log(n) + math.log(math.log(n)))/n\n",
    "            axes[nSamplesIdx].axhline(y=test_line, color='r', linestyle='--', label='Test Line')\n",
    "            colors = ['blue', 'orange']\n",
    "            for box, color in zip(bp['boxes'], colors):\n",
    "                box.set(color=color, linewidth=2)\n",
    "                box.set(facecolor='lightgrey')        \n",
    "            axes[nSamplesIdx].set_title(f'Janson test for LES, {Sample} points')\n",
    "            axes[nSamplesIdx].set_xticks([])\n",
    "        legend_colors = ['blue', 'orange']\n",
    "        legend_labels = ['QRNG', 'PRNG']\n",
    "        legend_patches = [mpatches.Patch(color=color, label=label) for color, label in zip(legend_colors, legend_labels)]\n",
    "        legend_elements = [Line2D([0], [0], color='red', lw=2, label='Median'), \n",
    "                        Line2D([0], [0], color='green', lw=2, linestyle='--', label='Mean')]\n",
    "        all_handles = legend_elements + legend_patches\n",
    "        plt.legend(handles=all_handles, loc='upper center', fontsize='small', bbox_to_anchor=(0.5, 0.0), ncol=4)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        file_path = os.path.join(Results_directory, 'Janson_' + precision+'.png')\n",
    "        plt.savefig(file_path, dpi=150)\n",
    "        plt.show()\n",
    "        \n",
    "        boxplot_statistic(Nsamples_number,arrays['qrng_les_means'], arrays['prng_les_means'], 'LES',precision,test)\n",
    "        \n",
    "    elif test=='nn':\n",
    "        ALL_tests_for_metric = run_statistical_tests(test,precision, Nsamples, arrays['qrng_nn_means'], arrays['prng_nn_means'],alpha)\n",
    "        boxplot_statistic(Nsamples_number,arrays['qrng_nn_means'], arrays['prng_nn_means'], 'NN',precision,test)\n",
    "    elif test=='PE_Q':\n",
    "        ALL_tests_for_metric =run_statistical_tests(test,precision, Nsamples, arrays['qrng_q_means'], arrays['prng_q_means'],alpha)\n",
    "        boxplot_statistic(Nsamples_number,arrays['qrng_q_means'], arrays['prng_q_means'], 'Q',precision,test)\n",
    "\n",
    "    elif test=='Avg':\n",
    "        ALL_tests_for_metric = run_statistical_tests(test,precision, Nsamples, arrays['qrng_Avg_means'], arrays['prng_Avg_means'],alpha)\n",
    "        boxplot_statistic(Nsamples_number,arrays['qrng_Avg_means'], arrays['prng_Avg_means'], r'\\bar\\gamma',precision,test)\n",
    "\n",
    "    elif test=='CoV':\n",
    "        ALL_tests_for_metric = run_statistical_tests(test,precision, Nsamples, arrays['qrng_CoV_means'], arrays['prng_CoV_means'],alpha)\n",
    "        boxplot_statistic(Nsamples_number,arrays['qrng_CoV_means'], arrays['prng_CoV_means'], 'C_V',precision,test)\n",
    "\n",
    "    elif test=='Rm':\n",
    "        #run_statistical_tests(test,precision, Nsamples, arrays['qrng_Rm_means'], arrays['prng_Rm_means'],alpha)\n",
    "        boxplot_statistic(Nsamples_number,arrays['qrng_Rm_means'], arrays['prng_Rm_means'], 'R_m',precision,test)\n",
    "        \n",
    "    elif test =='CE':\n",
    "        for Sample, nSamplesIdx in zip(Nsamples, range(len(Nsamples)) ):\n",
    "            n = float(Nsamples_number[nSamplesIdx])\n",
    "            E_gamma = 0.5 * pow(n, -1/2) + 4 * pow(n, -1) * (0.514 + 0.412 * pow(n, -1/2))\n",
    "            var_gamma = 0.070 * pow(n, -2) + 0.148 * pow(n, -5/2)\n",
    "            arrays['qrng_CE'][:, nSamplesIdx] =  (arrays['qrng_Avg_means'][:, nSamplesIdx] - E_gamma)/np.sqrt(var_gamma)\n",
    "            arrays['prng_CE'][:, nSamplesIdx] =  (arrays['prng_Avg_means'][:, nSamplesIdx] - E_gamma)/np.sqrt(var_gamma)\n",
    "        ALL_tests_for_metric = run_statistical_tests(test,precision, Nsamples, arrays['qrng_CE'], arrays['prng_CE'],alpha)\n",
    "        boxplot_statistic(Nsamples_number,arrays['qrng_CE'], arrays['prng_CE'], 'CE',precision,test)\n",
    "\n",
    "            \n",
    "    elif test =='R_nr':\n",
    "        for Sample, nSamplesIdx in zip(Nsamples, range(len(Nsamples)) ):\n",
    "            n = float(Nsamples_number[nSamplesIdx])\n",
    "            E_gamma = 0.5 * pow(n, -1/2) + 4 * pow(n, -1) * (0.514 + 0.412 * pow(n, -1/2))\n",
    "            arrays['qrng_R_nr'][:, nSamplesIdx] =  (arrays['qrng_Avg_means'][:, nSamplesIdx])/E_gamma\n",
    "            arrays['prng_R_nr'][:, nSamplesIdx] =  (arrays['prng_Avg_means'][:, nSamplesIdx])/E_gamma\n",
    "        \n",
    "        ALL_tests_for_metric = run_statistical_tests(test,precision, Nsamples, arrays['qrng_R_nr'], arrays['prng_R_nr'],alpha)\n",
    "        boxplot_statistic(Nsamples_number,arrays['qrng_R_nr'], arrays['prng_R_nr'], 'R_{nr}',precision,test)\n",
    "    return ALL_tests_for_metric\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c296bac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24508/2011340280.py:92: UserWarning: p-value capped: true value larger than 0.25\n",
      "  ad_result = stats.anderson_ksamp([case1_means, prngcase1_means])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Anderson_ksampResult' object has no attribute 'pvalue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m Dictionary_General\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test \u001b[38;5;129;01min\u001b[39;00m test_list:\n\u001b[0;32m----> 5\u001b[0m     ALL_tests_for_metric_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mstatistics_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     Dictionary_General[test] \u001b[38;5;241m=\u001b[39m ALL_tests_for_metric_tmp\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mstatistics_results\u001b[0;34m(test, alpha, precision)\u001b[0m\n\u001b[1;32m     66\u001b[0m         arrays[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqrng_R_nr\u001b[39m\u001b[38;5;124m'\u001b[39m][:, nSamplesIdx] \u001b[38;5;241m=\u001b[39m  (arrays[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqrng_Avg_means\u001b[39m\u001b[38;5;124m'\u001b[39m][:, nSamplesIdx])\u001b[38;5;241m/\u001b[39mE_gamma\n\u001b[1;32m     67\u001b[0m         arrays[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprng_R_nr\u001b[39m\u001b[38;5;124m'\u001b[39m][:, nSamplesIdx] \u001b[38;5;241m=\u001b[39m  (arrays[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprng_Avg_means\u001b[39m\u001b[38;5;124m'\u001b[39m][:, nSamplesIdx])\u001b[38;5;241m/\u001b[39mE_gamma\n\u001b[0;32m---> 69\u001b[0m     ALL_tests_for_metric \u001b[38;5;241m=\u001b[39m \u001b[43mrun_statistical_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNsamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqrng_R_nr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprng_R_nr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     boxplot_statistic(Nsamples_number,arrays[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqrng_R_nr\u001b[39m\u001b[38;5;124m'\u001b[39m], arrays[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprng_R_nr\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR_\u001b[39m\u001b[38;5;132;01m{nr}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,precision,test)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ALL_tests_for_metric\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mrun_statistical_tests\u001b[0;34m(test, precision, Nsamples, qrng_means, prng_means, alpha)\u001b[0m\n\u001b[1;32m     92\u001b[0m ad_result \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39manderson_ksamp([case1_means, prngcase1_means])\n\u001b[1;32m     93\u001b[0m ad_statistic \u001b[38;5;241m=\u001b[39m ad_result\u001b[38;5;241m.\u001b[39mstatistic\n\u001b[0;32m---> 94\u001b[0m p_value \u001b[38;5;241m=\u001b[39m \u001b[43mad_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpvalue\u001b[49m\n\u001b[1;32m     95\u001b[0m test_result_AD \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSignificant\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p_value \u001b[38;5;241m<\u001b[39m alpha \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot Significant\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     97\u001b[0m data_ADtest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP-value\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mformat\u001b[39m(p_value, Table_digits))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Anderson_ksampResult' object has no attribute 'pvalue'"
     ]
    }
   ],
   "source": [
    "test_list = {'nn','PE_Q','CoV','CE','R_nr','les'} ##test_list = {'les','nn','PE_Q','CoV','Rm','CE','R_nr','les'}\n",
    "\n",
    "Dictionary_General={}\n",
    "for test in test_list:\n",
    "    ALL_tests_for_metric_tmp = statistics_results(test,alpha,precision)\n",
    "    Dictionary_General[test] = ALL_tests_for_metric_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11184f9c",
   "metadata": {},
   "source": [
    "## All test results for a specific sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32846f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OneSample = \"50K\"\n",
    "nSamplesIdx = Nsamples.index(OneSample)\n",
    "\n",
    "\n",
    "#test_list = ['NN','Q','C_V','CE','R_{nr}'] \n",
    "test_list = ['NN','LES','Q','CE','R_{nr}'] \n",
    "\n",
    "\n",
    "QRNG_data_keys = ['qrng_nn_means', 'qrng_q_means', 'qrng_CoV_means', 'qrng_CE', 'qrng_R_nr']\n",
    "PRNG_data_keys = ['prng_nn_means', 'prng_q_means', 'prng_CoV_means', 'prng_CE', 'prng_R_nr']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(test_list), figsize=(12, 5))\n",
    "\n",
    "for testIdx, (qrng_key, prng_key) in enumerate(zip(QRNG_data_keys, PRNG_data_keys)):\n",
    "    qrng_data = arrays[qrng_key]  \n",
    "    prng_data = arrays[prng_key]  \n",
    "    bp = axes[testIdx].boxplot([qrng_data[:, nSamplesIdx], prng_data[:, nSamplesIdx]], patch_artist=True, showmeans=True, meanline=True, medianprops=dict(color='red'))\n",
    "    colors = ['blue', 'orange']\n",
    "    for box, color in zip(bp['boxes'], colors):\n",
    "            box.set(color=color, linewidth=2)\n",
    "            box.set(facecolor='lightgrey')        \n",
    "\n",
    "    axes[testIdx].set_title('${}$ test'.format(test_list[testIdx]))\n",
    "    axes[testIdx].set_xticks([])\n",
    "    \n",
    "legend_colors = ['blue', 'orange']\n",
    "legend_labels = ['QRNG', 'PRNG']\n",
    "legend_patches = [mpatches.Patch(color=color, label=label) for color, label in zip(legend_colors, legend_labels)]\n",
    "legend_elements = [Line2D([0], [0], color='red', lw=2, label='Median'), \n",
    "                Line2D([0], [0], color='green', lw=2, linestyle='--', label='Mean')]\n",
    "all_handles = legend_elements + legend_patches\n",
    "fig.subplots_adjust(bottom=0.1)  \n",
    "fig.legend(handles=all_handles, loc='lower center', fontsize='small', ncol=4, frameon=False, bbox_to_anchor=(0.5, -0.04))\n",
    "fig.text(0.01, -0.02, f\"{Nsamples_number[nSamplesIdx]} sample points of {precision} precision\", fontsize=10, ha='left', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "file_path = os.path.join(Results_directory, f'Test_summary_for_'+OneSample+'_'+precision+'_points.png')\n",
    "plt.savefig(file_path, dpi=150, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0056742",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = ['nn', 'PE_Q', 'CoV', 'CE', 'R_nr','les']\n",
    "columns = ['W test', 'T test', 'F test', 'AD test', 'KS test']\n",
    "\n",
    "df = pd.DataFrame(index=rows, columns=columns) \n",
    "for row in rows:\n",
    "    for column in columns:\n",
    "        if row in Dictionary_General and column in Dictionary_General[row]:\n",
    "            dict_temp = Dictionary_General[row].get(column, [None])[nSamplesIdx] \n",
    "            df.loc[row, column] = dict_temp \n",
    "print('Sample size = ', OneSample, '| precision = ', precision, '| alpha = ', alpha)\n",
    "print()\n",
    "print(df)\n",
    "\n",
    "\n",
    "file_path = Results_directory +'/'+ OneSample+'_'+precision+'_AllTests_table.txt'\n",
    "print(file_path)\n",
    "table = tabulate(df, headers='keys', tablefmt='pretty')\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(f\"alpha = {alpha}\\n\")        \n",
    "    file.write(f\"{OneSample}, {precision} \\n\")        \n",
    "    file.write(table)\n",
    "    file.write('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3a6ef-7952-492e-bf70-f64ac6fb9fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
